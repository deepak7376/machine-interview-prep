### Open-ended Questions:

1. **Question:**
   *The attention mechanism is a fundamental component of transformers. Can you explain how attention works in a transformer model and discuss any potential limitations or challenges associated with it? How might researchers address these challenges in future transformer architectures?*

### Coding Questions:

3. **Question: Implementing Transformer Encoder:**
   *Write a Python code snippet to implement a simple transformer encoder. Consider using PyTorch or TensorFlow, and include the necessary components such as multi-head self-attention, feedforward layer, layer normalization, etc. Provide a brief explanation of each component in your implementation.*
